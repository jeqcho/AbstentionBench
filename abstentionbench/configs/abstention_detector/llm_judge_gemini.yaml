# @package _global_

judge_name: LLMJudgeGemini

abstention_detector:
  _target_: recipe.evaluation.LLMJudgeAbstentionDetector
  use_gt_in_prompt: True
  judge_model:
    _target_: recipe.models.Gemini15ProAPI
    max_tokens: 100
    # Setting temp. to 0 means we are generating output as
    # arg max {P(yes | context), P(no | context)} in the judge.
    # This also follows the judge setup in prior works like
    # CoCoNot and HarmBench.
    temperature: 0.
  save_dir: ${save_dir}

abstention_detector_launcher:
  gpus_per_node: 1
  nodes: 1
  tasks_per_node: 1
  cpus_per_task: 8
  timeout_min: 400
  slurm_partition: devlab,learnlab,learnfair
  mem_gb: 480
  slurm_constraint: "volta32gb"
  exclude: "learnfair7486"
  slurm_srun_args: ["-vv", "--cpu-bind", "none"]
  comment: "eval step"
