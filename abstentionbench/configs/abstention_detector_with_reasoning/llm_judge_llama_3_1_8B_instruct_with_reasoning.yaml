# @package _global_

judge_with_reasoning_name: LLMJudge_Llama_3_1_8B_With_Reasoning

abstention_detector_with_reasoning:
  _target_: recipe.evaluation.LLMJudgeAbstentionDetectorWithReasoning
  judge_model:
    _target_: recipe.models.Llama3_1_8B_Instruct
    # Setting temp. to 0 means we are generating output as
    # arg max {P(yes | context), P(no | context)} in the judge.
    # This also follows the judge setup in prior works like
    # CoCoNot and HarmBench.
    temperature: 0.
    top_p: 0.95
    max_tokens: 100
    convert_prompt_to_chat: True
    tensor_parallel_size: 1
    # This context window is too small for a few (prompt, response) pairs, however,
    # we can't accommodate a larger context window on v100.
    max_model_len: 32768
  save_dir: ${save_dir}
